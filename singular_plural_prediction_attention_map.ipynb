{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b2f727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens import ActivationCache, HookedTransformer\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd6ab08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-XL into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-XL\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dad54ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-47): 48 x TransformerBlock(\n",
       "      (ln1): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36212329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerBlock(\n",
       "  (ln1): LayerNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (ln2): LayerNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (attn): Attention(\n",
       "    (hook_k): HookPoint()\n",
       "    (hook_q): HookPoint()\n",
       "    (hook_v): HookPoint()\n",
       "    (hook_z): HookPoint()\n",
       "    (hook_attn_scores): HookPoint()\n",
       "    (hook_pattern): HookPoint()\n",
       "    (hook_result): HookPoint()\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (hook_pre): HookPoint()\n",
       "    (hook_post): HookPoint()\n",
       "  )\n",
       "  (hook_attn_in): HookPoint()\n",
       "  (hook_q_input): HookPoint()\n",
       "  (hook_k_input): HookPoint()\n",
       "  (hook_v_input): HookPoint()\n",
       "  (hook_mlp_in): HookPoint()\n",
       "  (hook_attn_out): HookPoint()\n",
       "  (hook_mlp_out): HookPoint()\n",
       "  (hook_resid_pre): HookPoint()\n",
       "  (hook_resid_mid): HookPoint()\n",
       "  (hook_resid_post): HookPoint()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks = model.blocks\n",
    "blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f23ee67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookPoint()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks[0].attn.hook_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d42dd0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../dataset_csvs_singular_plural/predictions/correct_preds_xl_s_plural.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "849bd1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(data_path)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e83f1ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>snswer</th>\n",
       "      <th>predictions</th>\n",
       "      <th>probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The plural of cat is</td>\n",
       "      <td>cats</td>\n",
       "      <td>cats</td>\n",
       "      <td>0.526731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The plural of dog is</td>\n",
       "      <td>dogs</td>\n",
       "      <td>dogs</td>\n",
       "      <td>0.599398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The plural of book is</td>\n",
       "      <td>books</td>\n",
       "      <td>books</td>\n",
       "      <td>0.422725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                sentence snswer predictions  probabilities\n",
       "0   The plural of cat is   cats        cats       0.526731\n",
       "1   The plural of dog is   dogs        dogs       0.599398\n",
       "2  The plural of book is  books       books       0.422725"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6af172a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store hook_attn_scores values\n",
    "# for all datapoints , for all layers\n",
    "hook_attn_scores = []\n",
    "predictions = []\n",
    "probabilities = []\n",
    "k = 1\n",
    "\n",
    "# Define a function to store hook_attn_scores values\n",
    "def store_hook_attn_scores(module, input, output):\n",
    "    hook_attn_scores.append(output)\n",
    "\n",
    "# Register hooks for layers 6 to 11\n",
    "for layer in range(0, 26):\n",
    "    #blocks[layer].hook_attn_out.register_forward_hook(store_hook_attn_scores)\n",
    "    blocks[layer].attn.hook_pattern.register_forward_hook(store_hook_attn_scores)\n",
    "\n",
    "for data_i, row in data.iterrows():\n",
    "    print(data_i)\n",
    "    example_prompt = row['sentence']\n",
    "    example_answer = row['snswer']\n",
    "    tokens = model.to_tokens(example_prompt, prepend_bos=True)\n",
    "    logits, cache = model.run_with_cache(tokens) \n",
    "    year_probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "    topk = torch.topk(year_probs, k=k)\n",
    "    topk_tokens = [[model.tokenizer.decode(top) for top in ex] for ex in topk.indices]\n",
    "    probs_array = topk.values.cpu().detach().numpy()[0].tolist()\n",
    "    predictions.extend(topk_tokens[0])\n",
    "    probabilities.extend(probs_array)\n",
    "    words = ['bos']\n",
    "    w = example_prompt.split(' ')\n",
    "    words.extend(w)\n",
    "    for layer_i, layer in enumerate(hook_attn_scores):\n",
    "        for head_i, head in enumerate(layer[0]):\n",
    "            np.save('../attention_arrays/s_plural_datapoint_' + str(data_i) + '_' + str(layer_i) + '_' + str(head_i) + '.npy', head.detach().cpu().numpy())\n",
    "            \n",
    "    \n",
    "    hook_attn_scores.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1192dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0.5267309546470642]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store hook_attn_scores values\n",
    "hook_attn_scores = []\n",
    "predictions = []\n",
    "probabilities = []\n",
    "k = 1\n",
    "\n",
    "# Define a function to store hook_attn_scores values\n",
    "def store_hook_attn_scores(module, input, output):\n",
    "    hook_attn_scores.append(output)\n",
    "\n",
    "# Register hooks for layers 6 to 11\n",
    "for layer in range(6, 12):\n",
    "    #blocks[layer].hook_attn_out.register_forward_hook(store_hook_attn_scores)\n",
    "    blocks[layer].attn.hook_pattern.register_forward_hook(store_hook_attn_scores)\n",
    "\n",
    "for data_i, row in data.iterrows():\n",
    "    print(data_i)\n",
    "    example_prompt = row['sentence']\n",
    "    example_answer = row['snswer']\n",
    "    tokens = model.to_tokens(example_prompt, prepend_bos=True)\n",
    "    logits, cache = model.run_with_cache(tokens) \n",
    "    year_probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "    topk = torch.topk(year_probs, k=k)\n",
    "    topk_tokens = [[model.tokenizer.decode(top) for top in ex] for ex in topk.indices]\n",
    "    probs_array = topk.values.cpu().detach().numpy()[0].tolist()\n",
    "    predictions.extend(topk_tokens[0])\n",
    "    probabilities.extend(probs_array)\n",
    "    print(probabilities)\n",
    "    words = ['bos']\n",
    "    w = example_prompt.split(' ')\n",
    "    words.extend(w)\n",
    "    for layer_i, layer in enumerate(hook_attn_scores):\n",
    "        for head_i, head in enumerate(layer[0]):\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.title(f'Attention Map for Layer {layer_i} head {head_i}')\n",
    "\n",
    "            # Create the heatmap\n",
    "            heatmap = plt.imshow(head.detach().cpu().numpy(), aspect='auto', cmap='viridis')\n",
    "            plt.colorbar()\n",
    "\n",
    "            # Add text annotations for the attention values\n",
    "            for (i, j), val in np.ndenumerate(head.detach().cpu().numpy()):\n",
    "                plt.text(j, i, f'{val:.2f}', ha='center', va='center', color='white')\n",
    "\n",
    "            plt.xlabel('Tokens')\n",
    "            plt.xticks(ticks=range(len(words)), \n",
    "                    labels=words)\n",
    "            plt.ylabel('Tokens')\n",
    "            plt.yticks(ticks=range(len(words)), \n",
    "                    labels=words)\n",
    "\n",
    "            #plt.savefig('../feature_maps_analysis_plots/s_plural_gap_6_datapoint_' + str(data_i) + '_' + str(layer_i) + '_' + str(head_i) + '.png')\n",
    "            #np.save('../feature_maps_analysis_arrays/s_plural_gap_6_datapoint_' + str(data_i) + '_' + str(layer_i) + '_' + str(head_i) + '.npy', head.detach().cpu().numpy())\n",
    "            \n",
    "            plt.close()\n",
    "    break\n",
    "    hook_attn_scores.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34eb0d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficient: nan\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example attention values for each token (from a specific head and layer)\n",
    "attention_values = np.array([0.11, 0.15, 0.12, 0.1, 0.15, 0.1, 0, 0, 0])\n",
    "\n",
    "\n",
    "# Example output confidence values (logit or probability for the correct plural form)\n",
    "output_value = 0.8\n",
    "output_values = np.full(attention_values.shape, output_value)\n",
    "\n",
    "# Calculate the correlation coefficient\n",
    "correlation_coefficient = np.corrcoef(attention_values, output_values)[0, 1]\n",
    "\n",
    "print(f\"Correlation coefficient: {correlation_coefficient}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4516c01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficient: 3.2811686960750074e-16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example attention values for each token (from a specific head and layer)\n",
    "attention_values = np.array([0.11, 0.15, 0.12, 0.1, 0.15, 0.1, 0, 0, 0])\n",
    "\n",
    "# Remove all 0s from attention_values\n",
    "attention_values = attention_values[attention_values != 0]\n",
    "\n",
    "# Example output confidence values (logit or probability for the correct plural form)\n",
    "output_value = 0.8\n",
    "output_values = np.full(attention_values.shape, output_value)\n",
    "\n",
    "# Calculate the correlation coefficient\n",
    "correlation_coefficient = np.corrcoef(attention_values, output_values)[0, 1]\n",
    "\n",
    "print(f\"Correlation coefficient: {correlation_coefficient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15edf8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hook_attn_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2877bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_attn_scores[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb09a25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = hook_attn_scores[6][0][3]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(f'Attention Map for Layer 1 head 1')\n",
    "\n",
    "# Create the heatmap\n",
    "heatmap = plt.imshow(a.detach().cpu().numpy(), aspect='auto', cmap='viridis')\n",
    "plt.colorbar()\n",
    "\n",
    "# Add text annotations for the attention values\n",
    "for (i, j), val in np.ndenumerate(a.detach().cpu().numpy()):\n",
    "    plt.text(j, i, f'{val:.2f}', ha='center', va='center', color='white')\n",
    "\n",
    "plt.xlabel('Tokens')\n",
    "plt.xticks(ticks=range(len(['bos', 'the', 'plural', 'of', 'house', 'is'])), \n",
    "           labels=['bos', 'the', 'plural', 'of', 'house', 'is'])\n",
    "plt.ylabel('Tokens')\n",
    "plt.yticks(ticks=range(len(['bos', 'the', 'plural', 'of', 'house', 'is'])), \n",
    "           labels=['bos', 'the', 'plural', 'of', 'house', 'is'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90c92c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Distance: 21.64276123046875\n",
      "Cosine Similarity: 0.5845746994018555\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-xl')\n",
    "model = GPT2Model.from_pretrained('gpt2-xl')\n",
    "\n",
    "# Tokenize words and ensure they are in the format expected by the model\n",
    "tokens_house = tokenizer('mouse', return_tensors='pt')\n",
    "tokens_houses = tokenizer('mice', return_tensors='pt')\n",
    "\n",
    "# Extract embeddings\n",
    "with torch.no_grad():\n",
    "    outputs_house = model(tokens_house['input_ids'])\n",
    "    outputs_houses = model(tokens_houses['input_ids'])\n",
    "\n",
    "    # Assuming using only the last hidden state for embedding\n",
    "    emb_house = outputs_house.last_hidden_state.squeeze(0)  # Remove batch dimension\n",
    "    emb_houses = outputs_houses.last_hidden_state.squeeze(0)  # Remove batch dimension\n",
    "\n",
    "    # Mean over all token positions to get a single vector per input\n",
    "    emb_house_mean = torch.mean(emb_house, dim=0)\n",
    "    emb_houses_mean = torch.mean(emb_houses, dim=0)\n",
    "\n",
    "# Calculate Euclidean distance\n",
    "euclidean_distance = torch.norm(emb_house_mean - emb_houses_mean, p=2).item()\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_similarity = torch.nn.functional.cosine_similarity(emb_house_mean.unsqueeze(0), emb_houses_mean.unsqueeze(0), dim=1).item()\n",
    "\n",
    "print(f\"Euclidean Distance: {euclidean_distance}\")\n",
    "print(f\"Cosine Similarity: {cosine_similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9fd2c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7500000000000001"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.21 + 0.45 + 0.03 + 0.04 + 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fbbc6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Distance: 0.2232699990272522\n",
      "Cosine Similarity: 1.0\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([0.52673])\n",
    "b = torch.tensor([0.75])\n",
    "euclidean_distance = torch.norm(b - a, p=2).item()\n",
    "cosine_similarity = torch.nn.functional.cosine_similarity(b, a, dim=0).item()\n",
    "\n",
    "print(f\"Euclidean Distance: {euclidean_distance}\")\n",
    "print(f\"Cosine Similarity: {cosine_similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146d83e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
